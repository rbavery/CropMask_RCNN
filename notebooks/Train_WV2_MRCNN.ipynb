{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following setup draws from the Mask_RCNN repo by matterport and Deep Learning with Python by Chollet. \n",
    "\n",
    "https://github.com/matterport/Mask_RCNN\n",
    "https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "\n",
    "We import our packages, including maskrcnn, which needs to be installed from the github repo. \n",
    "\n",
    "We also set up our directories and paths before we organize our data into tensors. \n",
    "\n",
    "We subclass the dataset and config classes for our specific dataset\n",
    "\n",
    "Then, we train the model and test.\n",
    "\n",
    "TO DO:\n",
    "- Try to prepare the dataset and see if Keras trains succesfully, with loss decreasing at each step.\n",
    "\n",
    "- Try data augmentation: image rotation and flipping to increase our training set 6 fold\n",
    "\n",
    "- Explore data aug options in load_image_gt():  \n",
    "\n",
    "        augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.\n",
    "        For example, passing imgaug.augmenters.Fliplr(0.5) flips images\n",
    "        right/left 50% of the time.\n",
    "\n",
    "- change Config attributes to see if hyperparameters like anchor sizes (size of proposed regions that objects are located in) dramatically impact model training time and performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure that GPUs are detected. On Tana, default conda is only kenrel that works for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io as skio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import shutil\n",
    "from imgaug import augmenters as iaa\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "# Import Mask RCNN\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import log\n",
    "from mrcnn.parallel_model import ParallelModel\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"/home/rave/tana-crunch/waves/deepimagery/data/raw/wv2/\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"models\")\n",
    "\n",
    "# # Local path to trained weights file\n",
    "# COCO_MODEL_PATH = os.path.join(MODEL_DIR, \"mask_rcnn_coco.h5\")\n",
    "# # Download COCO trained weights from Releases if needed\n",
    "# if not os.path.exists(COCO_MODEL_PATH):\n",
    "#     utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, 'train')\n",
    "TEST_DIR = os.path.join(ROOT_DIR, 'test')\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "# Results directory\n",
    "# Save submission files here\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"results/\")\n",
    "\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "random.seed(4)\n",
    "\n",
    "def remove_dir_folders(directory):\n",
    "    folderlist = [ f for f in os.listdir(directory)]\n",
    "    for f in folderlist:\n",
    "        shutil.rmtree(os.path.join(TEST_DIR,f))\n",
    "\n",
    "def train_test_split(train_dir, test_dir, kprop):\n",
    "    \"\"\"Takes a sample of folder ids and copies them to a test directory. \n",
    "    each sample folder containes an images and corresponding masks folder\"\"\"\n",
    "    remove_dir_folders(test_dir)\n",
    "    remove_dir_folders(train_dir)\n",
    "    sample_list = next(os.walk(train_dir))[1]\n",
    "    k = round(kprop*len(sample_list))\n",
    "    test_list = random.sample(sample_list,k)\n",
    "    for test_sample in test_list:\n",
    "        shutil.copytree(os.path.join(train_dir,test_sample),os.path.join(test_dir,test_sample))\n",
    "    train_list = list(set(next(os.walk(train_dir))[1]) - set(test_list))\n",
    "    print(len(train_list))\n",
    "    print(len(test_list))\n",
    "    return train_list, test_list\n",
    "    \n",
    "train_list, test_list = train_test_split(TRAIN_DIR, TEST_DIR, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageryConfig(Config):\n",
    "    \"\"\"Configuration for training on worldview-2 imagery. \n",
    "    Will eventually want to make this a sub-class of a \n",
    "    larger Imagery class. Overrides values specific to WV2.\n",
    "    \n",
    "    Descriptive documentation for each attribute is at\n",
    "    https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/config.py\"\"\"\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \"\"\"Set values of computed attributes. Channel dimension is overriden, \n",
    "        replaced 3 with N as per this guideline: https://github.com/matterport/Mask_RCNN/issues/314\n",
    "        THERE MAY BE OTHER CODE CHANGES TO ACCOUNT FOR 3 vs N channels. See other \n",
    "        comments.\"\"\"\n",
    "        # https://github.com/matterport/Mask_RCNN/wiki helpful for N channels\n",
    "        # Effective batch size\n",
    "        self.BATCH_SIZE = self.IMAGES_PER_GPU * self.GPU_COUNT\n",
    "        \n",
    "        # Input image size\n",
    "        if self.IMAGE_RESIZE_MODE == \"crop\":\n",
    "            self.IMAGE_SHAPE = np.array([self.IMAGE_MIN_DIM, self.IMAGE_MIN_DIM, N])\n",
    "        else:\n",
    "            self.IMAGE_SHAPE = np.array([self.IMAGE_MAX_DIM, self.IMAGE_MAX_DIM, N])\n",
    "\n",
    "        # Image meta data length\n",
    "        # See compose_image_meta() for details\n",
    "        self.IMAGE_META_SIZE = 1 + 3 + 3 + 4 + 1 + self.NUM_CLASSES\n",
    "    \n",
    "    # LEARNING_RATE = .0001 \n",
    "    \n",
    "    # Image mean (RGBN RGBN) from WV2_MRCNN_PRE.ipynb\n",
    "    # filling with N values, need to compute mean of each channel\n",
    "    # values are for gridded wv2 no partial grids\n",
    "    MEAN_PIXEL = np.array([259.6, 347.0, 259.8, 416.3, 228.23, 313.4, 187.5, 562.9])\n",
    "    \n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"wv2-gridded-no-partial\"\n",
    "\n",
    "    # Batch size is 4 (GPUs * images/GPU).\n",
    "    # New parralel_model.py allows for multi-gpu\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + ag\n",
    "\n",
    "    # Use small images for faster training. Determines the image shape.\n",
    "    # From build() in model.py\n",
    "    # Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
    "    #     \"to avoid fractions when downscaling and upscaling.\"\n",
    "    #    \"For example, use 256, 320, 384, 448, 512, ... etc. \"\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small.\n",
    "    # Setting Large upper scale since some fields take up nearly \n",
    "    # whole image\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 200)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 50\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "    \n",
    "    #reduces the max number of field instances\n",
    "    MAX_GT_INSTANCES = 30\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 100\n",
    "    \n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101.\n",
    "    # You can also provide a callable that should have the signature\n",
    "    # of model.resnet_graph. If you do so, you need to supply a callable\n",
    "    # to COMPUTE_BACKBONE_SHAPE as well\n",
    "    BACKBONE = \"resnet50\"\n",
    "    \n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = False\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageryDataset(utils.Dataset):\n",
    "    \"\"\"Generates the Imagery dataset.\"\"\"\n",
    "    \n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Load the specified image and return a [H,W,8] Numpy array.\n",
    "        Channels are ordered [B, G, R, NIR]. This is called by the \n",
    "        Keras data_generator function\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        image = skio.imread(self.image_info[image_id]['path'])\n",
    "    \n",
    "        assert image.shape[-1] == 8\n",
    "        assert image.ndim == 3\n",
    "    \n",
    "        return image\n",
    "    \n",
    "    def load_wv2(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the nuclei dataset.\n",
    "\n",
    "        dataset_dir: Root directory of the dataset\n",
    "        subset: Subset to load.\n",
    "                * train: stage1_train excluding validation images\n",
    "                * val: validation images from VAL_IMAGE_IDS\n",
    "        \"\"\"\n",
    "        # Add classes. We have one class.\n",
    "        # Naming the dataset wv2, and the class agriculture\n",
    "        self.add_class(\"wv2\", 1, \"agriculture\")\n",
    "\n",
    "        assert subset in [\"train\", \"test\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "        if subset == \"test\":\n",
    "            image_ids = test_list\n",
    "        else:\n",
    "            image_ids = train_list\n",
    "        \n",
    "        # Add images\n",
    "        for image_id in image_ids:\n",
    "            self.add_image(\n",
    "                \"wv2\",\n",
    "                image_id=image_id,\n",
    "                path=os.path.join(dataset_dir, image_id, \"image/{}.tif\".format(image_id+'_OSGS_ms')))\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        # Get mask directory from image path\n",
    "        mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), \"masks\")\n",
    "\n",
    "        # Read mask files from .png image\n",
    "        mask = []\n",
    "        for f in next(os.walk(mask_dir))[2]:\n",
    "            if f.endswith(\".tif\"):\n",
    "                m = skio.imread(os.path.join(mask_dir, f)).astype(np.bool)\n",
    "                mask.append(m)\n",
    "                assert m.ndim == 2\n",
    "        mask = np.stack(mask, axis=-1)\n",
    "        assert mask.ndim == 3\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID, we return an array of ones\n",
    "        return mask, np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "    \n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"field\":\n",
    "            return info[\"id\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset_dir, subset):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Training dataset.\n",
    "    dataset_train = ImageryDataset()\n",
    "    dataset_train.load_wv2(dataset_dir, \"train\")\n",
    "    dataset_train.prepare()\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = ImageryDataset()\n",
    "    dataset_val.load_wv2(dataset_dir, \"test\")\n",
    "    dataset_val.prepare()\n",
    "\n",
    "    # Image augmentation\n",
    "    # http://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
    "    augmentation = iaa.SomeOf((0, 2), [\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Flipud(0.5),\n",
    "        iaa.OneOf([iaa.Affine(rotate=90),\n",
    "                   iaa.Affine(rotate=180),\n",
    "                   iaa.Affine(rotate=270)]),\n",
    "        iaa.Multiply((0.8, 1.5)),\n",
    "        iaa.GaussianBlur(sigma=(0.0, 5.0))\n",
    "    ])\n",
    "\n",
    "    # *** This training schedule is an example. Update to your needs ***\n",
    "\n",
    "    print(\"Train all layers\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=40,\n",
    "                augmentation=augmentation,\n",
    "                layers='all')\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  RLE Encoding\n",
    "############################################################\n",
    "\n",
    "def rle_encode(mask):\n",
    "    \"\"\"Encodes a mask in Run Length Encoding (RLE).\n",
    "    Returns a string of space-separated values.\n",
    "    \"\"\"\n",
    "    assert mask.ndim == 2, \"Mask must be of shape [Height, Width]\"\n",
    "    # Flatten it column wise\n",
    "    m = mask.T.flatten()\n",
    "    # Compute gradient. Equals 1 or -1 at transition points\n",
    "    g = np.diff(np.concatenate([[0], m, [0]]), n=1)\n",
    "    # 1-based indicies of transition points (where gradient != 0)\n",
    "    rle = np.where(g != 0)[0].reshape([-1, 2]) + 1\n",
    "    # Convert second index in each pair to lenth\n",
    "    rle[:, 1] = rle[:, 1] - rle[:, 0]\n",
    "    return \" \".join(map(str, rle.flatten()))\n",
    "\n",
    "def rle_decode(rle, shape):\n",
    "    \"\"\"Decodes an RLE encoded list of space separated\n",
    "    numbers and returns a binary mask.\"\"\"\n",
    "    rle = list(map(int, rle.split()))\n",
    "    rle = np.array(rle, dtype=np.int32).reshape([-1, 2])\n",
    "    rle[:, 1] += rle[:, 0]\n",
    "    rle -= 1\n",
    "    mask = np.zeros([shape[0] * shape[1]], np.bool)\n",
    "    for s, e in rle:\n",
    "        assert 0 <= s < mask.shape[0]\n",
    "        assert 1 <= e <= mask.shape[0], \"shape: {}  s {}  e {}\".format(shape, s, e)\n",
    "        mask[s:e] = 1\n",
    "    # Reshape and transpose\n",
    "    mask = mask.reshape([shape[1], shape[0]]).T\n",
    "    return mask\n",
    "\n",
    "\n",
    "def mask_to_rle(image_id, mask, scores):\n",
    "    \"Encodes instance masks to submission format.\"\n",
    "    assert mask.ndim == 3, \"Mask must be [H, W, count]\"\n",
    "    # If mask is empty, return line with image ID only\n",
    "    if mask.shape[-1] == 0:\n",
    "        return \"{},\".format(image_id)\n",
    "    # Remove mask overlaps\n",
    "    # Multiply each instance mask by its score order\n",
    "    # then take the maximum across the last dimension\n",
    "    order = np.argsort(scores)[::-1] + 1  # 1-based descending\n",
    "    mask = np.max(mask * np.reshape(order, [1, 1, -1]), -1)\n",
    "    # Loop over instance masks\n",
    "    lines = []\n",
    "    for o in order:\n",
    "        m = np.where(mask == o, 1, 0)\n",
    "        # Skip if empty\n",
    "        if m.sum() == 0.0:\n",
    "            continue\n",
    "        rle = rle_encode(m)\n",
    "        lines.append(\"{}, {}\".format(image_id, rle))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Detection\n",
    "############################################################\n",
    "\n",
    "def detect(model, dataset_dir, subset):\n",
    "    \"\"\"Run detection on images in the given directory.\"\"\"\n",
    "    print(\"Running on {}\".format(dataset_dir))\n",
    "\n",
    "    # Create directory\n",
    "    if not os.path.exists(RESULTS_DIR):\n",
    "        os.makedirs(RESULTS_DIR)\n",
    "    submit_dir = \"submit_{:%Y%m%dT%H%M%S}\".format(datetime.datetime.now())\n",
    "    submit_dir = os.path.join(RESULTS_DIR, submit_dir)\n",
    "    os.makedirs(submit_dir)\n",
    "\n",
    "    # Read dataset\n",
    "    dataset = ImageryDataset(8)\n",
    "    dataset.load_wv2(dataset_dir, subset)\n",
    "    dataset.prepare()\n",
    "    # Load over images\n",
    "    submission = []\n",
    "    for image_id in dataset.image_ids:\n",
    "        # Load image and run detection\n",
    "        image = dataset.load_image(image_id)\n",
    "        # Detect objects\n",
    "        r = model.detect([image], verbose=0)[0]\n",
    "        # Encode image to RLE. Returns a string of multiple lines\n",
    "        source_id = dataset.image_info[image_id][\"id\"]\n",
    "        rle = mask_to_rle(source_id, r[\"masks\"], r[\"scores\"])\n",
    "        submission.append(rle)\n",
    "        # Save image with masks. Only show first three bands\n",
    "        visualize.display_instances(\n",
    "            image[:,:,0:3], r['rois'], r['masks'], r['class_ids'],\n",
    "            dataset.class_names, r['scores'],\n",
    "            show_bbox=False, show_mask=False,\n",
    "            title=\"Predictions\")\n",
    "        plt.savefig(\"{}/{}.png\".format(submit_dir, dataset.image_info[image_id][\"id\"]))\n",
    "\n",
    "    # Save to csv file\n",
    "    submission = \"ImageId,EncodedPixels\\n\" + \"\\n\".join(submission)\n",
    "    file_path = os.path.join(submit_dir, \"submit.csv\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(submission)\n",
    "    print(\"Saved to \", submit_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model, trying without initial weights\n",
    "generate an empty mask for images without fields\n",
    "or\n",
    "toss images and masks where there are no fields (probably the worse option, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "config = ImageryConfig(8)\n",
    "config.display()\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                                  model_dir=MODEL_DIR)\n",
    "#model = ParallelModel(model, 2) doesn't work yet\n",
    "\n",
    "#cProfile.run('train(model, ROOT_DIR, \"train\")')\n",
    "train(model, ROOT_DIR, \"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = next(os.walk(TEST_DIR))[1]\n",
    "train_list = list(set(next(os.walk(TRAIN_DIR))[1]) - set(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageryInferenceConfig(ImageryConfig):\n",
    "    # Set batch size to 1 to run one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    # Don't resize imager for inferencing\n",
    "    IMAGE_RESIZE_MODE = \"pad64\"\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7\n",
    "model_run_folder = 'wv2-gridded-no-partial20180627T0722'\n",
    "weight_file = 'mask_rcnn_wv2-gridded-no-partial_0009.h5'\n",
    "weights_path = os.path.join(MODEL_DIR, model_run_folder, weight_file)\n",
    "iconfig = ImageryInferenceConfig(8)\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=iconfig,\n",
    "                                  model_dir=MODEL_DIR)\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "detect(model, ROOT_DIR, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
