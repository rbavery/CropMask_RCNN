{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "from skimage import measure\n",
    "from skimage import morphology as skim\n",
    "import skimage.io as skio\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import yaml\n",
    "import geopandas as gpd\n",
    "from rasterio import features, coords\n",
    "import rasterio\n",
    "from shapely.geometry import shape\n",
    "import gdal\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "def parse_yaml(input_file):\n",
    "    \"\"\"Parse yaml file of configuration parameters.\"\"\"\n",
    "    with open(input_file, 'r') as yaml_file:\n",
    "        params = yaml.load(yaml_file)\n",
    "    return params\n",
    "\n",
    "params = parse_yaml('preprocess_config.yaml') \n",
    "\n",
    "ROOT = params['dirs']['root']\n",
    "\n",
    "DATASET = os.path.join(\n",
    "    ROOT, params['dirs']['dataset'])\n",
    "\n",
    "REORDER = os.path.join(\n",
    "    DATASET, params['dirs']['reorder'])\n",
    "\n",
    "TRAIN = os.path.join(\n",
    "    DATASET, params['dirs']['train'])\n",
    "\n",
    "TEST = os.path.join(\n",
    "    DATASET, params['dirs']['test'])\n",
    "\n",
    "GRIDDED_IMGS = os.path.join(\n",
    "    DATASET, params['dirs']['gridded_imgs'])\n",
    "\n",
    "GRIDDED_LABELS = os.path.join(\n",
    "    DATASET, params['dirs']['gridded_labels'])\n",
    "\n",
    "OPENED = os.path.join(\n",
    "    DATASET, params['dirs']['opened'])\n",
    "\n",
    "NEG_BUFFERED = os.path.join(\n",
    "    DATASET, params['dirs']['neg_buffered_labels'])\n",
    "\n",
    "RESULTS = os.path.join(ROOT,'../',params['dirs']['results'], params['dirs']['dataset'])\n",
    "\n",
    "SOURCE_IMGS = os.path.join(\n",
    "    ROOT, params['dirs']['source_imgs'])\n",
    "\n",
    "SOURCE_LABELS = os.path.join(\n",
    "    ROOT, params['dirs']['source_labels'])\n",
    "\n",
    "def make_dirs():\n",
    "    \n",
    "    dirs = [DATASET, REORDER, TRAIN, TEST, GRIDDED_IMGS, GRIDDED_LABELS, OPENED, NEG_BUFFERED, RESULTS]\n",
    "\n",
    "    # Make directory and subdirectories\n",
    "    for d in dirs:\n",
    "        pathlib.Path(d).mkdir(parents=False, exist_ok=False)\n",
    "\n",
    "    # Change working directory to project directory\n",
    "    os.chdir(dirs[0])\n",
    "make_dirs()\n",
    "\n",
    "def yaml_to_band_index(params):\n",
    "    band_list = []\n",
    "    if params['image_vals']['dataset'] == 'landsat':\n",
    "        bands = params['landsat_bands_to_include']\n",
    "    elif params['image_vals']['dataset'] == 'wv2':\n",
    "        bands = params['wv2_bands_to_include']\n",
    "    for i, band in enumerate(bands):\n",
    "        if list(band.values())[0]== True:\n",
    "            band_list.append(i)\n",
    "    return band_list\n",
    "\n",
    "def reorder_images(params):\n",
    "    \"\"\"Load the os, gs, both, or any single date images and subset bands. Growing\n",
    "    Season is stacked first before OS if both true.\n",
    "    \"\"\"\n",
    "    file_ids_all = next(os.walk(SOURCE_IMGS))[2]\n",
    "    band_indices = yaml_to_band_index(params)\n",
    "    if params['seasons']['GS'] or params['seasons']['OS']:\n",
    "        image_ids_gs = sorted([image_id for image_id in file_ids_all \\\n",
    "                               if 'GS' in image_id and '.aux' not in image_id])\n",
    "        image_ids_os = sorted([image_id for image_id in file_ids_all \\\n",
    "                               if 'OS' in image_id and '.aux' not in image_id])\n",
    "    else:\n",
    "        image_ids = sorted([image_id for image_id in file_ids_all \\\n",
    "                               if '.tif' in image_id and '.aux' not in image_id])\n",
    "    \n",
    "    if params['seasons']['GS'] and params['seasons']['OS'] == False:\n",
    "        for img_path in image_ids_gs:\n",
    "            gs_image = skio.imread(os.path.join(SOURCE_IMGS, img_path))\n",
    "            gs_image = gs_image[:,:,band_indices]\n",
    "            skio.imsave(img_path, gs_image, plugin='tifffile')\n",
    "\n",
    "    elif params['seasons']['OS'] and params['seasons']['GS'] == False:\n",
    "        for img_path in image_ids_os:\n",
    "            os_image = skio.imread(os.path.join(SOURCE_IMGS, img_path))\n",
    "            os_image = gs_image[:,:,band_indices]\n",
    "            skio.imsave(img_path, os_image, plugin='tifffile')\n",
    "    elif params['seasons']['OS'] and params['seasons']['GS']:\n",
    "        for gs_path, os_path in zip(image_ids_gs, image_ids_os):\n",
    "            gs_image = skio.imread(os.path.join(SOURCE_IMGS, gs_path))\n",
    "            os_image = skio.imread(os.path.join(SOURCE_IMGS, os_path))\n",
    "            gsos_image = np.dstack([gs_image[:,:,band_indices], os_image[:,:,band_indices]])\n",
    "\n",
    "            match = SequenceMatcher(None, gs_path, os_path).find_longest_match(0, len(gs_path), 0, len(os_path))\n",
    "            path = gs_path[match.b: match.b + match.size] \n",
    "            # this may need to be reworked for diff file names\n",
    "            # works best if unique ids like GS go in front of filename\n",
    "            gsos_image_path = os.path.join(REORDER, path + 'OSGS.tif')\n",
    "            skio.imsave(gsos_image_path, gsos_image, plugin='tifffile')\n",
    "            \n",
    "    else: # for non wv2, single date case\n",
    "            for img_path in image_ids:\n",
    "                image = skio.imread(os.path.join(SOURCE_IMGS, img_path))\n",
    "                image = image[:,:,band_indices] # might be best to sav all image bands in each tiff with tile align notebook since we subset here\n",
    "                skio.imsave(img_path, image, plugin='tifffile')\n",
    "\n",
    "\n",
    "def negative_buffer_and_small_filter(params):\n",
    "    \"\"\"\n",
    "    Applies a negative buffer to labels since some are too close together and \n",
    "    produce conjoined instances when connected components is run (even after \n",
    "    erosion/dilation). This may not get rid of all conjoinments and should be adjusted.\n",
    "    It relies too on the source projection of the label file to calculate distances for\n",
    "    the negative buffer. Currently, if using wv2, labels are reprojected. for other imagery\n",
    "    it's assumed that the projection is in meters and that a negative buffer in meter units \n",
    "    will work with this projection.\n",
    "\n",
    "    Returns rasterized labels that are ready to be gridded\n",
    "    \n",
    "    The function is unfortunately split, one process if the data source is wv2 and one if it is landsat.\n",
    "    \"\"\"\n",
    "    \n",
    "    class_int = params['label_vals']['class']\n",
    "    neg_buffer = float(params['label_vals']['neg_buffer'])\n",
    "    small_area_filter = float(params['label_vals']['small_area_filter'])\n",
    "    big_area_filter = float(params['label_vals']['big_area_filter'])\n",
    "    # This is a helper  used with sorted for a list of strings by specific indices in \n",
    "    # each string. Was used for a long path that ended with a file name\n",
    "    # Not needed here but may be with different source imagery and labels\n",
    "    # def takefirst_two(elem):\n",
    "    #     return int(elem[-12:-10])\n",
    "\n",
    "    items = os.listdir(SOURCE_LABELS)\n",
    "    labels = []\n",
    "    for name in items:\n",
    "        if name.endswith(\".shp\") or name.endswith(\".geojson\"):\n",
    "            labels.append(os.path.join(SOURCE_LABELS,name))  \n",
    "\n",
    "    shp_list = sorted(labels)\n",
    "    # need to use Source imagery for geotransform data for rasterized shapes, didn't preserve when save imgs to reorder\n",
    "    scenes = os.listdir(SOURCE_IMGS)\n",
    "    \n",
    "    if params['image_vals']['dataset'] =='wv2':\n",
    "        #hard coded season because it will take more tinkering to have the model and preprocessing work with multichannel\n",
    "        scenes = [scene for scene in scenes if 'GS' in scene]\n",
    "        img_list = []\n",
    "        for name in scenes:\n",
    "            img_list.append(os.path.join(SOURCE_IMGS,name))  \n",
    "\n",
    "        img_list = sorted(img_list)\n",
    "\n",
    "\n",
    "        for shp_path, img_path in zip(shp_list, img_list):\n",
    "            shp_frame = gpd.read_file(shp_path)\n",
    "            # keeps the class of interest if it is there and the polygon of raster extent\n",
    "            shp_frame = shp_frame[(shp_frame['class'] == class_int) | (shp_frame['DN'] == 1)]\n",
    "            with rasterio.open(img_path) as rast:\n",
    "                meta = rast.meta.copy()\n",
    "                meta.update(compress=\"lzw\")\n",
    "                meta['count'] = 1\n",
    "            tifname = os.path.splitext(os.path.basename(shp_path))[0] + '.tif'\n",
    "            rasterized_name = os.path.join(NEG_BUFFERED, tifname)\n",
    "            with rasterio.open(rasterized_name, 'w+', **meta) as out:\n",
    "                out_arr = out.read(1)\n",
    "                # we get bounds to deterimine which projection to use for neg buffer\n",
    "                shp_frame.loc[0,'DN'] = 0\n",
    "                shp_frame.loc[1:,'DN'] = 1\n",
    "                maxx_bound = shp_frame.bounds.maxx.max()\n",
    "                minx_bound = shp_frame.bounds.minx.min()\n",
    "                #need to project to correct utm before buffering in units of meters, should move into seperate func\n",
    "                if maxx_bound >= 30 and minx_bound>= 30:\n",
    "                    shp_frame = shp_frame.to_crs({'init': 'epsg:32736'})\n",
    "                    shp_frame['geometry'] = shp_frame['geometry'].buffer(neg_buffer)\n",
    "                    shp_frame['Shape_Area'] = shp_frame.area\n",
    "                    shp_frame = shp_frame.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "                else:\n",
    "                    shp_frame = shp_frame.to_crs({'init': 'epsg:32735'})\n",
    "                    shp_frame['geometry'] = shp_frame['geometry'].buffer(neg_buffer)\n",
    "                    shp_frame['Shape_Area'] = shp_frame.area\n",
    "                    shp_frame = shp_frame.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "                if len(shp_frame) == 1: # added for case, where entire wv2 scenes have no foreground class and need empty masks\n",
    "                    shapes = ((geom,value) for geom, value in zip(shp_frame.geometry, shp_frame.DN))\n",
    "                    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform, default_value=1)\n",
    "                    burned[burned < 0] = 0\n",
    "                    out.write_band(1, burned)\n",
    "\n",
    "                else: # added for center pivot case, where entire wv2 scenes have no center pivots and need empty masks\n",
    "                    shp_frame = shp_frame.loc[shp_frame.Shape_Area > small_area_filter]\n",
    "                    shp_frame = shp_frame.loc[shp_frame.Shape_Area < big_area_filter]\n",
    "                    shp_frame = shp_frame[shp_frame.DN==1] # get rid of extent polygon\n",
    "                    # https://gis.stackexchange.com/questions/151339/rasterize-a-shapefile-with-geopandas-or-fiona-python#151861\n",
    "                    shapes = ((geom,value) for geom, value in zip(shp_frame.geometry, shp_frame.DN))\n",
    "                    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform, default_value=1)\n",
    "                    burned[burned < 0] = 0\n",
    "                    out.write_band(1, burned)\n",
    "        print('Done applying negbuff of {negbuff} and filtering small labels of area less than {area}'.format(negbuff=neg_buffer,area=small_area_filter))  \n",
    "    \n",
    "    elif params['image_vals']['dataset'] == 'landsat':\n",
    "        scenes = [scene for scene in scenes if '.tif' in scene and '.aux' not in scene]\n",
    "        img_list = []\n",
    "        for name in scenes:\n",
    "            img_list.append(os.path.join(SOURCE_IMGS,name))  \n",
    "\n",
    "        img_list = sorted(img_list)\n",
    "        for shp_path, img_path in zip(shp_list, img_list):\n",
    "            shp_frame = gpd.read_file(shp_path)\n",
    "            # keeps the class of interest if it is there and the polygon of raster extent\n",
    "            with rasterio.open(img_path) as rast:\n",
    "                meta = rast.meta.copy()\n",
    "                meta.update(compress=\"lzw\")\n",
    "                meta['count'] = 1\n",
    "                tifname = os.path.splitext(os.path.basename(shp_path))[0] + '.tif'\n",
    "                rasterized_name = os.path.join(NEG_BUFFERED, tifname)\n",
    "                with rasterio.open(rasterized_name, 'w+', **meta) as out:\n",
    "                    out_arr = out.read(1)\n",
    "                    shp_frame = shp_frame.loc[shp_frame.area > small_area_filter]\n",
    "                    shp_frame = shp_frame.loc[shp_frame.area < big_area_filter]\n",
    "                    shp_frame['geometry'] = shp_frame['geometry'].buffer(neg_buffer)\n",
    "                    # https://gis.stackexchange.com/questions/151339/rasterize-a-shapefile-with-geopandas-or-fiona-python#151861\n",
    "                    shapes = ((geom,value) for geom, value in zip(shp_frame.geometry, shp_frame.ObjectID))\n",
    "                    burned = features.rasterize(shapes=shapes, fill=0, out_shape=rast.shape, transform=out.transform, default_value=1)\n",
    "                    burned[burned < 0] = 0\n",
    "                    burned[burned > 0] = 1\n",
    "                    burned = burned.astype(np.int16, copy=False)\n",
    "                    out.write(burned, 1)\n",
    "        print('Done applying negbuff of {negbuff} and filtering small labels of area less than {area}'.format(negbuff=neg_buffer,area=small_area_filter)) \n",
    "\n",
    "def rm_mostly_empty(scene_path, label_path):\n",
    "    '''\n",
    "    Removes a grid that is mostly (over 1/4th) empty and corrects bad no data value to 0.\n",
    "    Ignor ethe User Warning, unsure why it pops up but doesn't seem to impact the array shape\n",
    "    '''\n",
    "    \n",
    "    usable_data_threshold = params['image_vals']['usable_thresh']\n",
    "    arr = skio.imread(scene_path)\n",
    "    arr[arr<0] = 0\n",
    "    skio.imsave(scene_path, arr)\n",
    "    pixel_count = arr.shape[0] * arr.shape[1]\n",
    "    nodata_pixel_count = (arr == 0).sum()\n",
    "    if 1-(nodata_pixel_count/pixel_count) < usable_data_threshold:\n",
    "\n",
    "        os.remove(scene_path)\n",
    "        os.remove(label_path)\n",
    "        print('removed scene and label, over {}% bad data'.format(usable_data_threshold))\n",
    "\n",
    "def grid_images(params):\n",
    "    \"\"\"\n",
    "    Grids up imagery to a variable size. Filters out imagery with too little usable data.\n",
    "    appends a random unique id to each tif and label pair, appending string 'label' to the \n",
    "    mask.\n",
    "    \"\"\"\n",
    "    if params['image_vals']['img_id'] is str:\n",
    "        img_list = [params['image_vals']['img_id']]\n",
    "        label_list = sorted(next(os.walk(NEG_BUFFERED))[2])\n",
    "        print(\"label list def for single id should change later to specifically reference the id!\")\n",
    "    else:\n",
    "        img_list = sorted(next(os.walk(REORDER))[2])\n",
    "        label_list = sorted(next(os.walk(NEG_BUFFERED))[2])\n",
    "    for img_name, label_name in zip(img_list, label_list):\n",
    "        img_path = os.path.join(REORDER, img_name)\n",
    "        label_path = os.path.join(NEG_BUFFERED, label_name)\n",
    "        #assign unique name to each gridded tif, keeping season suffix\n",
    "        #assigning int of same length as ZA0932324 naming convention\n",
    "        \n",
    "        tile_size_x = params['image_vals']['grid_size']\n",
    "        tile_size_y = params['image_vals']['grid_size']\n",
    "        ds = gdal.Open(img_path)\n",
    "        band = ds.GetRasterBand(1)\n",
    "        xsize = band.XSize\n",
    "        ysize = band.YSize   \n",
    "        \n",
    "        for i in range(0, xsize, tile_size_x):\n",
    "            for j in range(0, ysize, tile_size_y):\n",
    "                unique_id = str(random.randint(100000000,999999999))\n",
    "                out_path_img = os.path.join(GRIDDED_IMGS,unique_id)+ '.tif'\n",
    "                out_path_label = os.path.join(GRIDDED_LABELS,unique_id)+ '_label.tif'\n",
    "                com_string = \"gdal_translate -of GTIFF -srcwin \" + str(i)+ \", \" + \\\n",
    "                    str(j) + \", \" + str(tile_size_x) + \", \" + str(tile_size_y) + \" \" + \\\n",
    "                    str(img_path) + \" \" + str(out_path_img)\n",
    "                os.system(com_string)\n",
    "                com_string = \"gdal_translate -of GTIFF -srcwin \" + str(i)+ \", \" + \\\n",
    "                    str(j) + \", \" + str(tile_size_x) + \", \" + str(tile_size_y) + \" \" + \\\n",
    "                    str(label_path) + \" \" + str(out_path_label)\n",
    "                os.system(com_string)\n",
    "                rm_mostly_empty(out_path_img, out_path_label)\n",
    "\n",
    "def open_labels(params):\n",
    "    \"\"\"\n",
    "    Opens labels with kernel as defined in config.\n",
    "    \"\"\"\n",
    "    k = params['label_vals']['kernel']\n",
    "    label_list = next(os.walk(GRIDDED_LABELS))[2]\n",
    "    if params['label_vals']['open'] == True:\n",
    "        for name in label_list:\n",
    "            arr = skio.imread(os.path.join(GRIDDED_LABELS,name))\n",
    "            arr[arr < 0]=0\n",
    "            opened_path = os.path.join(OPENED,name)\n",
    "            kernel = np.ones((k,k))\n",
    "            arr = skim.binary_opening(arr, kernel)\n",
    "            arr=1*arr\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                skio.imsave(opened_path, 1*arr)\n",
    "\n",
    "        print('Done opening with kernel of h and w {size}'.format(size=k))\n",
    "\n",
    "    else:\n",
    "        for name in label_list:\n",
    "            arr = skio.imread(os.path.join(GRIDDED_LABELS,name))\n",
    "            arr[arr < 0]=0\n",
    "            opened_path = os.path.join(OPENED,name)\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                skio.imsave(opened_path, 1*arr)    \n",
    "\n",
    "def move_img_to_folder(params):\n",
    "    '''Moves a file with identifier pattern 760165086_OSGS.tif \n",
    "    (or just760165086.tif) to a \n",
    "    folder path ZA0165086/image/ZA0165086.tif\n",
    "    Also creates a mask folder at ZA0165086/masks\n",
    "    '''\n",
    "    \n",
    "    image_list = os.listdir(GRIDDED_IMGS)\n",
    "    for img in image_list:\n",
    "\n",
    "        folder_name = os.path.join(TRAIN,img[:9])\n",
    "        os.mkdir(folder_name)\n",
    "        new_path = os.path.join(folder_name, 'image')\n",
    "        mask_path = os.path.join(folder_name, 'mask')\n",
    "        os.mkdir(new_path)\n",
    "        file_path = os.path.join(GRIDDED_IMGS,img)\n",
    "        os.rename(file_path, os.path.join(new_path, img[:9]+'.tif'))\n",
    "        os.mkdir(mask_path)\n",
    "\n",
    "def connected_comp(params):\n",
    "    \"\"\"\n",
    "    Extracts individual instances into their own tif files. Saves them\n",
    "    in each folder ID in train folder. If an image has no instances,\n",
    "    saves it with a empty mask.\n",
    "    \"\"\"\n",
    "    label_list = next(os.walk(OPENED))[2]\n",
    "    # save connected components and give each a number at end of id\n",
    "    for name in label_list:\n",
    "        arr = skio.imread(os.path.join(OPENED,name))\n",
    "        blob_labels = measure.label(arr, background=0)\n",
    "        blob_vals = np.unique(blob_labels)\n",
    "        #for imgs with no isntances, create empty mask\n",
    "        if len(blob_vals)==1:\n",
    "            img_folder = os.path.join(TRAIN,name[:9], 'image')\n",
    "            img_name = os.listdir(img_folder)[0]\n",
    "            img_path = os.path.join(img_folder, img_name)\n",
    "            arr = skio.imread(img_path)\n",
    "            mask = np.zeros_like(arr[:,:,0])\n",
    "            mask_folder = os.path.join(TRAIN,name[:9], 'mask')\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                label_stump = os.path.splitext(os.path.basename(name))[0]\n",
    "                skio.imsave(os.path.join(mask_folder,  label_stump + '_0.tif'),mask)\n",
    "        # only run connected comp if there is at least one instance\n",
    "        for blob_val in blob_vals[blob_vals!=0]:\n",
    "            labels_copy = blob_labels.copy()\n",
    "            labels_copy[blob_labels!=blob_val] = 0\n",
    "            labels_copy[blob_labels==blob_val] = 1\n",
    "\n",
    "            label_stump = os.path.splitext(os.path.basename(name))[0]\n",
    "            label_name = label_stump+'_'+str(blob_val)+'.tif'\n",
    "            mask_path = os.path.join(TRAIN,name[:9], 'mask')\n",
    "            label_path = os.path.join(mask_path,label_name)\n",
    "            assert labels_copy.ndim == 2\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                skio.imsave(label_path, labels_copy)\n",
    "\n",
    "def train_test_split(params):\n",
    "    \"\"\"Takes a sample of folder ids and copies them to a test directory\n",
    "    from a directory with all folder ids. Each sample folder contains an \n",
    "    images and corresponding masks folder.\"\"\"\n",
    "\n",
    "    k = params['image_vals']['split']\n",
    "    sample_list = next(os.walk(TRAIN))[1]\n",
    "    k = round(k*len(sample_list))\n",
    "    test_list = random.sample(sample_list,k)\n",
    "    for test_sample in test_list:\n",
    "        shutil.copytree(os.path.join(TRAIN,test_sample),os.path.join(TEST,test_sample))\n",
    "    train_list = list(set(next(os.walk(TRAIN))[1]) - set(next(os.walk(TEST))[1]))\n",
    "    train_df = pd.DataFrame({'train': train_list})\n",
    "    test_df = pd.DataFrame({'test': test_list})\n",
    "    train_df.to_csv(os.path.join(RESULTS, 'train_ids.csv'))\n",
    "    test_df.to_csv(os.path.join(RESULTS, 'test_ids.csv'))\n",
    "\n",
    "def get_arr_channel_mean(channel):\n",
    "    \"\"\"\n",
    "    Calculate the mean of a given channel across all training samples.\n",
    "    \"\"\"\n",
    "     \n",
    "    means = []\n",
    "    train_list = list(set(next(os.walk(TRAIN))[1]) - set(TEST))\n",
    "    for i, fid in enumerate(train_list):\n",
    "        im_folder = os.path.join(TRAIN,fid, 'image')\n",
    "        im_path = os.path.join(im_folder, os.listdir(im_folder)[0])\n",
    "        arr = skio.imread(im_path)\n",
    "        arr = arr.astype(np.float32,copy=False)\n",
    "        # added because no data values different for wv2 and landsat, need to exclude from mean\n",
    "        nodata_value = arr.min() if arr.min() < 0 else -9999 \n",
    "        arr[arr==nodata_value]=np.nan\n",
    "        means.append(np.nanmean(arr[:,:,channel]))\n",
    "    print(np.mean(means))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    make_dirs()\n",
    "    reorder_images(params)\n",
    "    negative_buffer_and_small_filter(params)\n",
    "    grid_images(params)\n",
    "    open_labels(params)\n",
    "    move_img_to_folder(params)\n",
    "    connected_comp(params)\n",
    "    train_test_split(params)\n",
    "    print('preprocessing complete, ready to run model.')\n",
    "\n",
    "    print('channel means, put these in model_configs.py subclass')\n",
    "    band_indices = yaml_to_band_index(params)\n",
    "    for i, v in enumerate(band_indices):\n",
    "        get_arr_channel_mean(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12555.537\n",
      "8340.699\n",
      "8314.736\n"
     ]
    }
   ],
   "source": [
    "band_indices = yaml_to_band_index(params)\n",
    "for i, v in enumerate(band_indices):\n",
    "    get_arr_channel_mean(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
